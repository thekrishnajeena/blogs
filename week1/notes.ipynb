{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa280877",
   "metadata": {},
   "source": [
    "# Building a FeedForward network\n",
    "\n",
    "1. The INput Layer\n",
    "    - receives the data and just pass values along.\n",
    "\n",
    "2. The Hidden Layers\n",
    "    - the networks's computational engine\n",
    "    - they learn increasingly abstract features\n",
    "\n",
    "3. The Output Layer\n",
    "    - produces the final result\n",
    "\n",
    "Feedforward flow: information moves in one direction only, input to output(no loops)\n",
    "\n",
    "Fully Connected: every neuron connects to every neuron in the next layer.\n",
    "\n",
    "## The Standard Forward Model: A neuron's calculation\n",
    "\n",
    "Step 1: The Linear Part(Weighted Sum)\n",
    "\n",
    "    - a weighted sum of all inputs is calculated\n",
    "    - a bias term is added to this sum\n",
    "    - result: z = (Summation of wi*xi) + b\n",
    "\n",
    "Step 2: The Non-Linear Part(activation)\n",
    "    - the result z is passed through a non-linear activation function g.\n",
    "    - this introduces complexity and allows the network to learn non-linear patterns.\n",
    "    - final output: a = g(z)\n",
    "\n",
    "\n",
    "## The Forward Pass: The vectorized layer view\n",
    "\n",
    "- Problem: Calculating neuron by neuron is incredibly inefficient.\n",
    "- Solution: compute an entire layer at once using vectorized operations(i.e., matrix mul)\n",
    "\n",
    "![image.png](../public/images/image.png)\n",
    "\n",
    "![image2.png](../public/images/image2.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb75e445",
   "metadata": {},
   "source": [
    "# Activation Functions\n",
    "\n",
    "### The Problem: Stacking Linear Layers is Useless\n",
    "\n",
    "- Without a non-linear activation function, a deep network simply collapses into a single linear model, no matter how many layers it has.\n",
    "\n",
    "- With Non Linearity(g): \n",
    "    - the equation becomes: g(W2g(W1x+b1) + b2)\n",
    "    - this function cannot be simplified.\n",
    "    - allows network to learn arbitrarily complex pattern.\n",
    "\n",
    "    - Tradition Activations: Sigmoid\n",
    "        - simga(z) = 1/(1+e^(-z))\n",
    "        - squeezes any real number into the range (0,1).\n",
    "        - historically popular for its interpretation as a neuron's firing rate.\n",
    "        - useful in output layers for binary classification(predicting probabilities).\n",
    "\n",
    "        **Problems**: \n",
    "            - Vanishing Gradients: the function is flat at both ends. the gradient is near zero for large positive or negative inputs, which effectively stops learning in deep networks.\n",
    "            - not zero-centered: outputs are always positive, which can slow down learning.\n",
    "\n",
    "    - Tanh\n",
    "        - tanh(z) = (e^z - e^(-z))/(e^z + e^(-z))\n",
    "        - squeezes any real number into the range (-1, 1).\n",
    "        - **ZERO-Centered**: its major advantage over sigmoid, this property helps center the data for the next layer, often speeding up convergence.\n",
    "\n",
    "        **Problems**:\n",
    "            - Vanishing gradients, difficult to use in deep networks.\n",
    "\n",
    "    - ReLU\n",
    "        - ReLU(z) = max(0,z)\n",
    "        - computationally efficient(just a threshold)\n",
    "        - no Vanishing Gradient (for z>0): gradient is a constant 1 for positive inputs, allowing learning signals to propagate deep into the network.\n",
    "        - Sparsity: by outputting 0 for negative inputs, can make the network sparse.\n",
    "\n",
    "        **Problems**:\n",
    "            - The Dying ReLU problem: if a neuron's weights are upated such that its pre-activation z is always negative, it will always output 0. The gradient will also be 0, and the neuron can never recover. It effectively \"dies\".\n",
    "\n",
    "        **Fixing The Dying Problem**:\n",
    "            - Leaky ReLU:\n",
    "                - f(z) = { z if z>0, alpha*z if z<=0 where alpha is a small constant like 0.01\n",
    "                - introducing a small, non-zero gradient for negative inputs.\n",
    "                - allows \"dead\" neurons to be revived, a very common and effective choice.\n",
    "\n",
    "        **GELU(Gaussian Error Linear Unit)**:\n",
    "            - a smoother, probabilistic alternative to ReLU\n",
    "            - became the standard in state-of-the-art Transformer models(e.g., GPT, BERT).\n",
    "\n",
    "- choosing activations:\n",
    "    **For Hidden Layers**\n",
    "    - start with ReLU\n",
    "    - for issues with dying neurons, switch to Leaky ReLU or Parametric ReLU.\n",
    "    - for transformer based models, consider GELU or Swish.\n",
    "\n",
    "    **For Output Layer(Task Dependent)**\n",
    "    \n",
    "        - Binary Classification: Sigmoid(outputs a probability between 0 and 1)\n",
    "        - Multiclass Classification: Softmax(outputs a probability distribution over all classes(all outputs sum to 1))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
